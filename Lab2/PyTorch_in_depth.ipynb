{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Machine Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn the basics of *Machine Learning* (ML) through hands-on practice. We cover the following topics:\n",
    "\n",
    "1. Frameworks: PyTorch\n",
    "3. Model Archetypes: Support Vector Machines, Linear Regression, Logistic Regression, Multi-Layer Perceptrons, Convolutional Neural Networks.\n",
    "4. Performance metrics: Accuracy, Confusion Matrix.\n",
    "5. Model Validation Techniques: Cross-Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to PyTorch\n",
    "\n",
    "Let's start by importing all the necessary libraries.\n",
    "\n",
    "*__Note:__ Don't forget to install PyTorch using the instructions on their website! This is done to ensure you have proper GPU acceleration. More info in the Installation instructions on Studium.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tensors and Their Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, what is a *tensor*? Tensors are *multi-dimensional arrays*, together with some rules on how to do math with them. \n",
    "\n",
    "NumPy's `ndarray` is one example of a way to represent tensors in Python, and PyTorch `tensor`s are another. Both approaches look quite similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([3.0])  # 1D array with a single entry: 3\n",
    "b = torch.zeros([3, 2])  # 3x2 matrix filled with zeros\n",
    "c = torch.ones_like(b)  # b-sized matrix filled with ones\n",
    "d = torch.ones_like(a)  # a-sized matrix filled with zeros\n",
    "\n",
    "a, b, c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many 'shortcut' functions to create a tensor. See the docs for more info: [PyTorch - Creation Ops](https://pytorch.org/docs/stable/torch.html#tensor-creation-ops)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key difference between NumPy's `ndarray` and PyTorch `tensor` is that the latter can run both on a CPU and on a GPU while ndarray can only run on CPUs. This gives a big advantage in case of heavy computations, as doing them on a GPU is usually much faster.\n",
    "\n",
    "However, PyTorch does not allocate tensors 'smartly' itself. Instead, you need to manually specify at the time of creation (or later on) on which device (CPU or GPU) you want to allocate the tensor. By default, it gets allocated on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, discover if the current computer can use GPU acceleration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Also works with ROCm (PyTorch > 1.8, Linux)\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "# Let's create a tensor on the available device\n",
    "torch.tensor([\n",
    "    [0, 1],\n",
    "    [2, 3]\n",
    "], device=device)\n",
    "\n",
    "# We can also move that tensor manually afer creation\n",
    "A = torch.tensor([\n",
    "    [0, 1],\n",
    "    [2, 3]\n",
    "])  # By default, it is allocated on the CPU\n",
    "print(f\"Before moving the tensor: {A}\")\n",
    "A = A.to(device)  # Will return a 'new' tensor, does not change A unless we re-assign it\n",
    "print(f\"After moving the tensor: {A}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tensor Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we learned how to allocate tensors on GPUs for super-fast computation, let's see how can actually handle the tensors (note how for our first, simpler exercises we will not bother moving tensors to GPU for the sake of readability).\n",
    "\n",
    "Here is a non-exhaustive list of examples. Some of them have a really similar syntax to NumPy (the full list can be found in the [official docs](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html#operations-on-tensors)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[3.0, 5.0]])\n",
    "b = torch.tensor([[-4.0, 4.0]])\n",
    "W = torch.tensor([\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0]\n",
    "])\n",
    "\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"W: {W}\")\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "# Slicing is very similar to NumPy:\n",
    "print(f\"Second column in W: {W[:, 1]}\")\n",
    "\n",
    "# torch.cat() joins tensors along an axis:\n",
    "print(f\"Stacking a and b:\\n {torch.cat([a, b], dim=0)}\")\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "# We can use artihmetic operators:\n",
    "\n",
    "print(f\"a + b: {a + b}\")  # element-wise a + b\n",
    "print(f\"a - b: {a - b}\")  # element-wise a - b\n",
    "print(f\"a * b: {a * b}\")  # element-wise a * b (!! different from matrix multiplication !!)\n",
    "print(f\"a / b: {a / b}\")  # element-wise a/b\n",
    "print(f\"a @ W: {a @ W}\")  # matrix multiplication\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "# For each operator above, there is a function that does the same thing:\n",
    "print(f\"torch.matmul(a, W): {torch.matmul(a, W)}\")  # same as 'a @ W'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, you can already do a lot of operations. Here is an example of a linear function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1.0, 1.0, 1.0])\n",
    "W = torch.tensor([\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0]\n",
    "])  # Identity matrix\n",
    "b = torch.tensor([0.0, 1.0, 2.0])\n",
    "\n",
    "Y_pred = X @ W + b\n",
    "Y_pred.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above is more than just a demonstration of PyTorch syntax: It is also a machine learning model called a *linear model*. We conceptualize it as a function $f_{W,b}(X) = Y_\\mathrm{pred}$: the vector $X$ is the model's input, $Y_\\mathrm{pred}$ is the model's output, and all other variables (here, $W$ and $b$) are the *model parameters*. The idea is that we can use some kind of automatic procedure to choose values for $W$ and $b$, so that $f_{W, b}()$ gives good approximate solutions for a problem. We often group all model paramters together into a variable called $\\theta$. \n",
    "\n",
    "For this linear model, the output $Y_\\mathrm{pred}$ is a continous number (making it a *regression model*), and the input $X$ is a *feature vector*. The process we performed above (taking a set of model parameters, taking one or more feature vectors, and computing the model's output) is called *prediction*.\n",
    "\n",
    "A single *feature vector* (like $X$ above) is usually interpreted as a row-vector (notice that we multiplied it *to the left* of `W`). This is not standard mathematical notation (we would usually represent $X$ as a column-vector that goes *to the right* of `W`), but is very common in Machine Learning. The reason is that we can pile up samples like in a spreadsheet, so each row is a different sample and each column is a different feature. We don't have to change any math to calculate several samples at once: the rules of matrix multiplication guarantee that each row is calculated separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Calculate\n",
    "$$ l = (Y_\\textrm{pred}-Y)^2 $$\n",
    "for the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = torch.tensor([1.0])\n",
    "Y = torch.tensor([1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Tip:__ Python's power syntax works out of the box with PyTorch.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Y_pred - Y) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Note:__ The expected output is* `tensor([0.2500])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function you just implemented is called the *squared error* and measures how well the model's output $Y_\\mathrm{pred}$ fits the desired output $Y$. The closer the prediction gets to the real value, the smaller $l$ gets. Averaging over a collection of samples (a dataset), we obtain the *Mean Square Error* (MSE)\n",
    "\n",
    "$$ L = \\textrm{mean}_i \\left( l^{(i)} \\right) = \\frac{1}{N} \\sum_{i=1}^N \\left( Y^{(i)}_\\textrm{pred}-Y^{(i)} \\right)^2, $$\n",
    "\n",
    "where $N$ is the number of samples, and the superindex $(i)$ indicates \"pertaining to the $i$-th sample\". MSE is one way of measuring a model's *goodness of fit* (how well a model fits the data) and is used for regession models. We call it (and other functions like it) a *loss function*, and to highlight this we sometimes call MSE the *MSE loss*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Building upon Exercise 1, compute the MSE loss for these inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0]])\n",
    "Y = torch.tensor([[1.5], [3.0], [2.5], [4.0], [6.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Tip:__ PyTorch tensors have a `mean()` method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((Y_pred - Y) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Note:__ The expected output of this function is* `tensor(0.5000)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the loss of regression models is nice, but what about classification models? What if we want to predict if a certain value is *true* or *false*, e.g., face vs. no face? As it turns out, we can turn the classification problem into a regression problem with a simple trick: Assume that the current sample has some probability to be in each category, and build a regression model that predicts the probability for each category. Then, predict the sample to be in the category with the highest probability.\n",
    "\n",
    "In a binary scenario (face, no face) we can create an even simpler regression model: We have the model return large positive values when it thinks the value is *true* (face), and large negative values when it thinks the value is *false* (no face). For ease of interpretability, we often convert the resulting score into a 0 to 1 range, because this allows us to interpret the result as a probability. For this, we need an S-shaped function that is close to 0 for large negative values, and close to 1 for large positive values. Anything with this shape is called a *sigmoid function*. We usually choose a sigmoid function called the *logistic function*:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1+e^{-z}}, $$\n",
    "\n",
    "operating element-by-element if $z$ is an array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Calculate the logistic function $\\sigma(z)$ for the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = torch.tensor([1.0, 2.0, 3.0, 4.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Tip:__ `torch.exp()` might be useful in this case.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / (1 + torch.exp(-Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Note:__ The expected output is* `tensor([0.7311, 0.8808, 0.9526, 0.9820])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "Calculate\n",
    "$$ l = \\max(0, 1- Y \\cdot Y_\\textrm{pred}) $$\n",
    "for the input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = torch.tensor([0.5])\n",
    "Y = torch.tensor([-1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Tip:__ there probably is a built in function for max, isn't there?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.maximum(torch.zeros(1), 1 - Y * Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Note:__ The expected output is:* `tensor([1.5000])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above is called [hinge loss](https://en.wikipedia.org/wiki/Hinge_loss). It is sometimes used as a loss when training binary classification models, and it follows the same idea of assigning positive $Y$ values to the category *true* (face) and negative $Y$ values to the category *false* (no face). It is most known in the context of *Support Vector Machines* (SVMs). Similar to the MSE loss earlier, we can use the hinge loss to compute a model's goodness of fit:\n",
    "\n",
    "$$ L = \\textrm{mean} \\left( l^{(i)} \\right) = \\frac{1}{N} \\sum_{i=1}^N \\max \\left( 0, 1 - Y^{(i)} \\cdot Y^{(i)}_\\textrm{pred} \\right). $$\n",
    "\n",
    "*__Note__: When using the hinge loss, the ground truth label $Y$ is chosen as $-1$ or $1$. Many other categorical losses choose $0$ or $1$ instead.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "\n",
    "Create an expression that computes the hinge loss for the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = torch.tensor([[1.0], [0.5], [0.0], [-0.5], [-1.0]])\n",
    "Y = torch.tensor([[1.0], [-1.0], [1.0], [-1.0], [1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.maximum(torch.zeros(1), 1 - Y * Y_pred).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Note:__ The expected output is* `tensor(1.)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "\n",
    "Create the expression for the following two functions using PyTorch.\n",
    "\n",
    "Function 1 is called `sigmoid`, takes $X$ as input, and has the form\n",
    "$$\\textrm{sigmoid}(X) = \\frac{1}{1+e^{-X}}$$\n",
    "\n",
    "Function 2 is called `Y_pred`, takes as input $X$, $W$, $b$, and applies the linear transformation from the example above followed by the sigmoid from function 1. It has the form\n",
    "$$Y_{pred}(X) = \\textrm{sigmoid}(X \\cdot W + b)$$\n",
    "\n",
    "Then, compute `Y_pred(X, W, b)`.\n",
    "\n",
    "This time, the inputs are a fake dataset of 5 examples with 2 features each, a weight matrix, and a constant (bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([\n",
    "    [0.5, 1],\n",
    "    [0.5, 2],\n",
    "    [0.5, 3],\n",
    "    [0.5, 4],\n",
    "    [0.001, 0.1]\n",
    "])\n",
    "W = torch.tensor([\n",
    "    [0.5],\n",
    "    [-0.5]\n",
    "])\n",
    "b = torch.tensor([0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1 / (1 + torch.exp(-X))\n",
    "\n",
    "def Y_pred(X, W, b):\n",
    "    return sigmoid(X @ W + b)\n",
    "\n",
    "Y_pred(X, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Note:__ The expected output is* \n",
    "```\n",
    "tensor([[0.4626],\n",
    "        [0.3430],\n",
    "        [0.2405],\n",
    "        [0.1611],\n",
    "        [0.5126]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $Y_\\textrm{pred}$ is a machine learning model. It is called *perceptron*, it is a binary *classification model*, and it takes the same approach to classification that we already encountered above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Computing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the pieces in place to begin looking into training models. For this, we rely on *automatic differentiation*, which is arguably the most important feature of PyTorch. Differentiation with respect to a parameter allows us to compute how the model's output will change if we change this parameter. We call the result a gradient and write $\\nabla_\\theta Y_\\textrm{pred}$ (the gradient of $Y_\\textrm{pred}$ with respect to $\\theta$). As the model's output affects the goodness of fit score, we can get a sense of how this score will change if we change a parameter of the model. Automatic differentiation then allows us to delegate this task to the computer.\n",
    "\n",
    "Armed with *automatic differentiation*, we can work out how to change the model's parameters $\\theta$ such that the goodness of fit improves for a given set of input features and target labels, i.e., our training set. A caveat of this process is that the gradient is only accurate in a small area around the current value of $\\theta$. Hence, instead of updating to the perfect set of model parameters in one shot, we need to do this iteratively and in small steps. We call this algorithm *gradient descent* as we use the gradient to *descend* in small steps to the optimal values of $\\theta$ to minimize a certain function (hence the name descent) and maximize the goodness of fit measure as a result. \n",
    "\n",
    "In PyTorch, computing the gradient via automatic differentiation looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.tensor([\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [-1.0],\n",
    "    [1.0]\n",
    "])  # expected output\n",
    "\n",
    "X = torch.tensor([\n",
    "    [0.5, 1],\n",
    "    [0.5, 2],\n",
    "    [0.5, 3],\n",
    "    [0.5, 4],\n",
    "    [0.001, 0.1]\n",
    "])  # input\n",
    "\n",
    "W = torch.tensor([\n",
    "    [0.5],\n",
    "    [-0.5]\n",
    "], requires_grad=True)  # we want to optimize the weights\n",
    "b = torch.tensor([0.1])  # bias\n",
    "\n",
    "# prediction\n",
    "Y_pred = X @ W + b  # linear model\n",
    "\n",
    "# loss (i.e. difference between expected output and actual output)\n",
    "loss = ((Y_pred - Y) ** 2).mean()  # MSE loss\n",
    "\n",
    "# let's now compute the gradients\n",
    "loss.backward()\n",
    "\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch records all the operations done on a vector and, from that, works out how to compute the gradients and stores them in the `grad` attribute. For this, it needs to know the variables with respect to which we want to compute gradients later. We set `requires_grad=True` to tell PyTorch to watch the operations done on that particular tensor. You can disable the automatic tracking for some operations with `torch.no_grad` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.tensor([\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [-1.0],\n",
    "    [1.0]\n",
    "])  # expected output\n",
    "X = torch.tensor([\n",
    "    [0.5, 1],\n",
    "    [0.5, 2],\n",
    "    [0.5, 3],\n",
    "    [0.5, 4],\n",
    "    [0.001, 0.1]\n",
    "])  # input\n",
    "W = torch.tensor([\n",
    "    [0.5],\n",
    "    [-0.5]\n",
    "], requires_grad=True)  # we want to optimize the weights\n",
    "b = torch.tensor([0.1])  # bias\n",
    "\n",
    "# prediction\n",
    "Y_pred = X @ W  # this operation is tracked\n",
    "\n",
    "with torch.no_grad():\n",
    "    a = Y_pred + b  # this operation is not tracked\n",
    "\n",
    "@torch.no_grad()\n",
    "def no_grad_fun(A, B):  # using this function will not affect the gradient\n",
    "    return A @ B\n",
    "\n",
    "print(Y_pred.detach())  # use detach to get a tensor that is detached from the gradients' graph\n",
    "\n",
    "# loss (i.e. difference between expected output and actual output)\n",
    "loss = ((Y_pred - Y) ** 2).mean()  # MSE loss\n",
    "\n",
    "# let's now compute the gradients\n",
    "loss.backward()\n",
    "\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using gradient descent we can find the minimum of a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the minimum of y = x^2\n",
    "def function_to_minimize(X):\n",
    "    return X * X\n",
    "\n",
    "X = torch.tensor(5.0, requires_grad=True)\n",
    "step_size = 0.1  # also called learning rate\n",
    "n_steps = 25  # change this value to get more or less close to the minimum\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # run the function\n",
    "    y = function_to_minimize(X)\n",
    "    \n",
    "    X.grad = None  # reset last step gradients (if any)\n",
    "    y.backward()  # compute gradients\n",
    "    \n",
    "    with torch.no_grad():  # don't record the update to the gradient\n",
    "        X -= step_size * X.grad  # update X using the gradient \n",
    "\n",
    "    print(f\"[{step+1:>2}] Minimized value: {function_to_minimize(X).detach().numpy():9.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the algorithm updates `X` in such a way that `function_to_minimize(X)` gets smaller and smaller. With this, we can train our first model in PyTorch!\n",
    "\n",
    "PyTorch also offers an automatic way of updating our models' parameters: Optimizers. An optimizer takes as input the parameters we want to optimize (e.g. `X` in the previous example) and does the optimization for us. In the following example we use the *Stochastic Gradient Descent* (SGD) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the minimum of y = x^2\n",
    "def function_to_minimize(X):\n",
    "    return X * X\n",
    "\n",
    "X = torch.tensor(5.0, requires_grad=True)\n",
    "step_size = 0.1\n",
    "n_steps = 25  # change this value to get more or less close to the minimum\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    [X],  # the parameter(s) to optimize\n",
    "    lr=step_size\n",
    ")\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # run the function\n",
    "    y = function_to_minimize(X)\n",
    "    \n",
    "    optimizer.zero_grad()  # reset old gradient\n",
    "    y.backward()  # compute gradients\n",
    "    optimizer.step()  # optimize parameters (i.e. X)\n",
    "\n",
    "    print(f\"[{step+1:>2}] Minimized value: {function_to_minimize(X).detach().numpy():9.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "\n",
    "Create a function called `mse_loss` that takes as input $Y_{pred}$ and $Y$ and computes the MSE loss. Then, create a function called `objective_function` that takes as input $X$, $Y$, $W$, $b$, and outputs the result of\n",
    "\n",
    "```\n",
    "mse_loss(linear_model(X, W, b), Y)\n",
    "```\n",
    "\n",
    "where `linear_model()` is `X @ W + b`. Finally, print the output of `objective_function(X_train, Y_train, W, b)` for the variables defined below using `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small training dataset of 10 examples\n",
    "\n",
    "X_train = torch.tensor([\n",
    "    [-1.],\n",
    "    [-0.77777778],\n",
    "    [-0.55555556],\n",
    "    [-0.33333333],\n",
    "    [-0.11111111],\n",
    "    [0.11111111],\n",
    "    [0.33333333],\n",
    "    [0.55555556],\n",
    "    [0.77777778],\n",
    "    [1.]\n",
    "])\n",
    "Y_train = torch.tensor([\n",
    "    [1.0319852],\n",
    "    [1.4628717],\n",
    "    [1.9185644],\n",
    "    [2.3697991],\n",
    "    [2.8011405],\n",
    "    [3.228254 ],\n",
    "    [3.6749988],\n",
    "    [4.1398144],\n",
    "    [4.6044483],\n",
    "    [5.0241356]\n",
    "])\n",
    "\n",
    "# parameters of the model\n",
    "W = torch.tensor([[0.5]], requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(Y, Y_pred):\n",
    "    return torch.mean((Y_pred - Y) ** 2)\n",
    "\n",
    "def linear_model(X, W, b):\n",
    "    return X @ W + b\n",
    "\n",
    "def objective_function(X, Y, W, b):\n",
    "    return mse_loss(linear_model(X, W, b), Y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(objective_function(X_train, Y_train, W, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Note:__ The expected output is* `tensor(10.0723)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8\n",
    "\n",
    "Use gradient descent *without* using `torch.optim` module to optimize the `objective_function(X_train, Y_train, W, b)`. Run the algorithm for $250$ steps and with a learning rate of $0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "for _ in range(250):\n",
    "    loss = objective_function(X_train, Y_train, W, b)\n",
    "    \n",
    "    W.grad = None\n",
    "    b.grad = None\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W -= lr * W.grad\n",
    "        b -= lr * b.grad\n",
    "\n",
    "W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Note:__ The expected values of W and b should be close to*\n",
    "```\n",
    "(tensor([[1.8069]], requires_grad=True),\n",
    " tensor(3.0062, requires_grad=True))\n",
    "```\n",
    "The optimal values for this problem are $W = 2$ and $b = 3$; training has come pretty close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9\n",
    "\n",
    "Now, achieve the same but by using `torch.optim.SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialise the model parameters\n",
    "W = torch.tensor([[0.5]], requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "optim = torch.optim.SGD([W, b], lr=lr)\n",
    "\n",
    "for _ in range(250):\n",
    "    loss = objective_function(X_train, Y_train, W, b)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have implemented linear regression from scratch using PyTorch. This is important, because the majority of currently used machine learning algorithms follow the same recipe used here:\n",
    "\n",
    "1. pick a model to use (here: linear model)\n",
    "2. pick a goodness of fit measure / loss function (here: MSE loss)\n",
    "3. pick an optimization algorithm (here: stochastic gradient descent)\n",
    "4. Use the chosen optimizer to update the model parameters and improve the chosen goodness of fit measure.\n",
    "5. Repeat 4 an (arbitrarily chosen) number of times. (I recommend blood sacrifices to the RNG gods when choosing a number.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 A Toy Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have built a first regression model; next up is building a (binary) classification model. For this, we will use a *synthetic* dataset (generated fake data) for which `sklearn.datasets` offers nice utilities. In particular, `make_classification()` allows us to generate a dataset with two classes called `Class A` and `Class B`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn to generate synthetic data.\n",
    "X, Y = make_classification(\n",
    "    n_samples=150, \n",
    "    n_features=2, \n",
    "    n_redundant=0, \n",
    "    n_clusters_per_class=1, \n",
    "    flip_y=0,\n",
    "    class_sep=2.0,\n",
    "    shuffle=True,\n",
    "    random_state=1337\n",
    ")\n",
    "\n",
    "# sklearn uses labels {0, 1}. Change them to {-1, +1} for the hinge loss.\n",
    "Y = (2 * Y - 1)\n",
    "\n",
    "Y = Y.astype(np.float32)\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "X_train, Y_train = torch.tensor(X[:100,:]), torch.tensor(Y[:100],).reshape(-1, 1)\n",
    "X_test, Y_test = torch.tensor(X[100:,:]), torch.tensor(Y[100:]).reshape(-1, 1)\n",
    "\n",
    "X_train[:5], Y_train[:5]  # show some training samples and their true label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our job now is to build a model that can predict which class each sample belongs to. This should work for both our training data (`X_train`) as well as our test data (`X_test`). We will begin by visualizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "class_a_train = (np.arange(len(X)) < 100) & (Y == -1)\n",
    "class_a_test = (np.arange(len(X)) >= 100) & (Y == -1)\n",
    "\n",
    "class_b_train = (np.arange(len(X)) < 100) & (Y == 1)\n",
    "class_b_test = (np.arange(len(X)) >= 100) & (Y == 1)\n",
    "\n",
    "ax.set_title(\"Fire & Ice\")\n",
    "ax.scatter(*X[class_a_train, :].T, c=\"tab:blue\")\n",
    "ax.scatter(*X[class_a_test, :].T, c=\"tab:cyan\")\n",
    "ax.scatter(*X[class_b_train, :].T, c=\"tab:red\")\n",
    "ax.scatter(*X[class_b_test, :].T, c=\"tab:orange\")\n",
    "ax.legend([\"Class A Train\", \"Class A Test\", \"Class B Train\", \"Class B Test\"])\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the figure above, the two classes are cleanly separated and the distribution of training and test data aligns in for both classes. Hence, we would expect the model to do quite well on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10\n",
    "\n",
    "Implement the following functions:\n",
    "\n",
    "1. A function called `hinge_loss` that takes $Y_{pred}$ and $Y$ as input and returns the expected value of the hinge loss (as encountered in Exercise 5).\n",
    "2. A function called `model` that takes $X$, $W$, $b$ as input and computes `linear_model(X, W, b)`. \n",
    "3. A function called `objective_function` that takes $X$, $Y$, $W$, $b$ as input and computes\n",
    "```\n",
    "hinge_loss(model(X, W, b), Y)\n",
    "```\n",
    "\n",
    "Finally, apply the `objective_function` to the training dataset generated above using `objective_function(X_train, Y_train, W, b)`.\n",
    "\n",
    "These model parameters for this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor([\n",
    "    [0.593],\n",
    "    [0.236]\n",
    "], requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(Y_pred, Y):\n",
    "    return torch.maximum(torch.zeros(1), 1 - Y_pred * Y).mean()\n",
    "\n",
    "def model(X, W, b):\n",
    "    return linear_model(X, W, b)\n",
    "\n",
    "def objective_function(X, Y, W, b):\n",
    "    return hinge_loss(model(X, W, b), Y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(objective_function(X_train, Y_train, W, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Note:__ The expected output is* `tensor(2.1518)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 11\n",
    " \n",
    "Similar to Exercise 9, find values for $W$ and $b$ that minimize `objective_function()`. Do this using `SGD` with a constant stepsize (learning rate) `lr=0.01` and perform 250 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD([W, b], lr=lr)\n",
    "\n",
    "for _ in range(250):\n",
    "    loss = objective_function(X_train, Y_train, W, b)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Note:__ After the optimzation the values of W and b should be*\n",
    "```\n",
    "(tensor([[-0.8273],\n",
    "         [ 0.0832]], requires_grad=True),\n",
    " tensor(-0.0955, requires_grad=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! You have just built and trained a linear SVM (*support vector machine*) in PyTorch from scratch. \n",
    "\n",
    "The next step is to investigate the performance of the model. For this, we need to recall how the output of the model relates to each class: We assigned `Class A` the label `1`, so whenever our model predicts a positive number, we predict that the sample belongs to `Class A`. Similarly, we assigned `Class B` the label `-1`, and whenever our model predicts a negative number, we predict that the sample belongs to `Class B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Use no_grad(). Otherwise, evaluation steps will be added to the gradient.\n",
    "    # First check the training accuracy\n",
    "    correctly_classified_examples = model(X_train, W, b).sign() == Y_train\n",
    "    correct_percent = correctly_classified_examples.count_nonzero().item() / len(Y_train)\n",
    "    print(f\"Training accuracy is: {correct_percent*100:.2f}%\")\n",
    "    \n",
    "    # then the test accuracy\n",
    "    correctly_classified_examples = model(X_test, W, b).sign() == Y_test\n",
    "    correct_percent = correctly_classified_examples.count_nonzero().item() / len(Y_test)\n",
    "    print(f\"Test accuracy is: {correct_percent*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect score!\n",
    "\n",
    "In addition to the numerical accuracy, we can visualize the so called *decision boundary* of our model. The decision boundary is the line at which our guess switches from one category (`Class A`) to the other (`Class B`). This is useful in understanding how the classifier decides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "class_a_train = (np.arange(len(X)) < 100) & (Y == -1)\n",
    "class_a_test = (np.arange(len(X)) >= 100) & (Y == -1)\n",
    "\n",
    "class_b_train = (np.arange(len(X)) < 100) & (Y == 1)\n",
    "class_b_test = (np.arange(len(X)) >= 100) & (Y == 1)\n",
    "\n",
    "ax.set_title(\"Fire & Ice\")\n",
    "ax.scatter(*X[class_a_train, :].T, c=\"tab:blue\")\n",
    "ax.scatter(*X[class_a_test, :].T, c=\"tab:cyan\")\n",
    "ax.scatter(*X[class_b_train, :].T, c=\"tab:red\")\n",
    "ax.scatter(*X[class_b_test, :].T, c=\"tab:orange\")\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")\n",
    "\n",
    "points_x = np.linspace(-0.5, 0)\n",
    "\n",
    "# always remember to detach tensors that require_grad\n",
    "points_y = -(b.detach() + W.detach()[0] * points_x) / W.detach()[1]\n",
    "ax.plot(points_x, points_y, linestyle=\"--\", color=\"red\")\n",
    "ax.legend([\"Class A Train\", \"Class A Test\", \"Class B Train\", \"Class B Test\", \"Decision Boundary\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to the left of the red line are predicted to be `Class B`; points to the right of the red line are predicted to be `Class A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, and as you have witnessed during the two models we have trained so far, training follows a fairly simple recipe. PyTorch picks up on that and ships with a high level module under `torch.nn`. It contains many handy functions and building blocks to make models and neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model Building API\n",
    "\n",
    "To build a model, we start by creating a class that inherits `torch.nn.Module`. This way, when we train and use the object, PyTorch will know automatically which function to call, which parameters to train, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # here define the strcuture of the model e.g. :\n",
    "        # a linear model similar to what we had earlier\n",
    "        self.simple_network = nn.Linear(in_features=2, out_features=1)\n",
    "    \n",
    "    # here define the 'forward' step i.e. compute the output of the model\n",
    "    def forward(self, x):\n",
    "        return self.simple_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Linear` takes a first argument `in_features` that indicates the dimension of the input vector and another `out_features` that indicates the dimension of the output vector. We can also specify whether we want a bias, the $b$ vector we used earlier, with the `bias` argument (which is `True` by default). We can call each building block of our model as if it was a function that takes an input and returns the output of that block e.g. `self.simple_network(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try using the model\n",
    "model = MyModel()\n",
    "\n",
    "with torch.no_grad():\n",
    "    Y_pred = model(X_train)\n",
    "\n",
    "X_train[:5], Y_pred[:5]  # show some training samples and relative predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside `nn` we can also find some common loss functions. For example, we can use the MSE loss by either calling `nn.functional.mse_loss(Y_pred, Y_train)`, or by creating a loss object with `loss_fn = nn.MSELoss()`, and then using it like `loss = loss_fn(Y_pred, Y_train)`.\n",
    "\n",
    "Let's now see a complete SVM example using these high-level APIs.\n",
    "\n",
    "*__Note__: PyTorch doesn't give us an implementation of the hinge loss out-of-the-box due to its simplicity. We can however find implementations of more complex losses. For the following example, to get the same results as before, we use our defined function for `hinge_loss`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train the model\n",
    "\n",
    "# we can automatically get the model parameters with its own function\n",
    "optim = torch.optim.SGD(params=model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"[ Parameters Before Training ]\")\n",
    "\n",
    "# let's also see the parameters values before we begin\n",
    "for a, b in model.named_parameters():\n",
    "    print(a, b)\n",
    "\n",
    "print(\"----\")\n",
    "print(\"[ Training ]\")\n",
    "\n",
    "loss_fn = hinge_loss\n",
    "\n",
    "for step in range(250):\n",
    "    Y_pred = model(X_train)\n",
    "    loss = loss_fn(Y_pred, Y_train)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if step % 25 == 0:\n",
    "        print(f\"[{step:>3}] Loss: {loss.item():9.6f}\")\n",
    "\n",
    "\n",
    "print(\"----\")\n",
    "print(\"[ Parameters After Training ]\")\n",
    "\n",
    "# let's also see the parameters values after the training\n",
    "for a, b in model.named_parameters():\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the model exactly as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # use no grad\n",
    "    correctly_classified_examples = model(X_train).sign() == Y_train\n",
    "    correct_percent = correctly_classified_examples.count_nonzero().item() / len(Y_train)\n",
    "    print(f\"Training accuracy is: {correct_percent*100:.2f}%\")\n",
    "    \n",
    "    # then the test accuracy\n",
    "    correctly_classified_examples = model(X_test).sign() == Y_test\n",
    "    correct_percent = correctly_classified_examples.count_nonzero().item() / len(Y_test)\n",
    "    print(f\"Test accuracy is: {correct_percent*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is virtually no difference between the SVM that you trained above, and the SVM trained here. The `nn` API simply saves you time and produces more readable code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 12\n",
    "\n",
    "Train a new model on the fake data using the `nn` API. \n",
    "\n",
    "**Step 1**: Build the model.\n",
    "\n",
    "1. Create a class called `LogisticRegressor` that inherits `nn.Module`.\n",
    "2. Create a `Linear` layer with the same shape as before.\n",
    "3. Create a `Sigmoid` layer. In this case, the sigmoid is our *activation function*, a function that we use after each layer of our network (only one in this case) to improve its performace.\n",
    "4. In the forward function feed the output of the linear layer to the sigmoid and return the result\n",
    "\n",
    "**Step 2**: Create the training loop. \n",
    "\n",
    "Use the optimizer `SGD` with learning rate 0.01 and the loss `BCELoss` (binary cross-entropy). Run the training for 200 steps.\n",
    "\n",
    "**Step 3**: Evaluate the model.\n",
    "\n",
    "Evaluate the accuracy of the model on the test data $X_{test}$, $Y_{test}$. Note that how you test the accuracy is different than before due to the addition of the sigmoid layer. Before, you had to check the sign of the output to check the predicted class but now your output is constrained between 0 and 1. In this case, we can set a threshold value (e.g. 0.5) to distinguish between the classes and then compare that to the ground truth as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Build the model\n",
    "class LogisticRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "\n",
    "model = LogisticRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Create the training loop\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "for _ in range(200):\n",
    "    Y_pred = model(X_train)\n",
    "    \n",
    "    loss = loss_fn(Y_pred, Y_train)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Evaluate the model\n",
    "with torch.no_grad():\n",
    "    correct_percent = torch.eq(model(X_train) > 0.5, Y_train > 0).count_nonzero() / len(Y_train)\n",
    "    print(f\"Training accuracy is: {correct_percent*100:.2f}%\")\n",
    "\n",
    "    correct_percent = torch.eq(model(X_test) > 0.5, Y_test > 0).count_nonzero() / len(Y_test)\n",
    "    print(f\"Test accuracy is: {correct_percent*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Note:__ Both accuracies should be above 90%*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! You have just implemented logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 13\n",
    "\n",
    "Create a *multi-layer perceptron* (MLP) and train it on the data used before following the steps below. A MLP is a simple neural network that consists of two logistic regression models stacked sequentially.\n",
    "\n",
    "**Step 1**: Build the model.\n",
    "\n",
    "1. Create a class called `MLP` that inherits `nn.Module`.\n",
    "2. Create 2 `Linear` layers: one with the same input shape as before and 500 as output one that takes 500 input and outputs the same shape as before.\n",
    "3. Create a `Sigmoid` layer to be applied after each `Linear` layer.\n",
    "4. In the forward function, feed the output of the first linear layer to the second layer, and return the result (don't forget to also apply the sigmoid!).\n",
    "\n",
    "**Step 2**: Create the training loop. \n",
    "\n",
    "Use the optimizer `SGD` with learning rate 0.01 and the loss `BCELoss` (binary cross-entropy). Run the training for 200 steps.\n",
    "\n",
    "**Step 3**: Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Build the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(2, 500)\n",
    "        self.linear2 = nn.Linear(500, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        first_result = self.sigmoid(self.linear1(x))\n",
    "        final_result = self.sigmoid(self.linear2(first_result))\n",
    "        return final_result\n",
    "\n",
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Create the training loop\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "for _ in range(200):\n",
    "    Y_pred = model(X_train)\n",
    "    \n",
    "    loss = loss_fn(Y_pred, Y_train)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Evaluate the model\n",
    "with torch.no_grad():\n",
    "    correct_percent = torch.eq(model(X_train) > 0.5, Y_train > 0).count_nonzero() / len(Y_train)\n",
    "    print(f\"Training accuracy is: {correct_percent*100:.2f}%\")\n",
    "\n",
    "    correct_percent = torch.eq(model(X_test) > 0.5, Y_test > 0).count_nonzero() / len(Y_test)\n",
    "    print(f\"Test accuracy is: {correct_percent*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Note:__ Both accuracies should be above 90%*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Real World Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have used *synthetic data*, artificially generated through the `sklearn` utilities. Separating two blobs of points is a good first example, but not very similar to real-world problems.\n",
    "\n",
    "Thus, we will now move on to widely available datasets collected from the real world. A very popular example is [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html), a collection of images, stored as 32x32px in 3-channel colors, that can be classified among ten categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck. Since the dataset is very common and often used as a benchmark in Machine Learning, it is easily available through PyTorch utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True)\n",
    "test_data = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images look something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()\n",
    "fig, ax_matrix = plt.subplots(2, 4)\n",
    "\n",
    "for idx in range(8):\n",
    "    ax = ax_matrix[idx // 4, idx % 4]\n",
    "    ax.imshow(train_data.data[idx])\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Note__: Actual real world data is messy. It needs labeling, cleaning and preprocessing before it can be used for ML. CIFAR10, and all other benchmark datasets, have this done already. This is great for learning how algorithms work, but bad for learning how to do ML in the wild. We will, hence, revisit data cleaning during Assignment 1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the dataset we just downloaded is not ready to use as it is now. Each data point (image) is stored as a `PILImage`, while, of course, we want to work with tensors. In the following code, we use `torchvision.transforms` to achieve this, and we also normalize the values inside. This module can also apply many different transformations that may come in handy depending on the dataset e.g. `CenterCrop`, `Resize`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(  # Do multiple transformations\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data.transform = transform\n",
    "test_data.transform = transform\n",
    "\n",
    "transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real world datasets are often too big to store in the computer's main memory, much less in the GPU's memory. Even if we manage to fit the samples in memory, when we train we need to keep a lot of extra state. This means that we often need to work in smaller *batches* (groups of samples), loading data on demand from disk, and discarding each batch before we load the next. With an added bit of random shuffling, this turns out to be so good for training that people use batches even if the dataset is small enough to fit in memory.\n",
    "\n",
    "PyTorch provides utilities for loading data and batching it in the module `torch.utils.data`. Two important classes are `Dataset` and `DataLoader`.\n",
    "\n",
    "* `Dataset` objects like `train_data` and `test_data` handle grabbing the data from disk or elsewhere; in the case of `CIFAR10` they get it directly from RAM (because CIFAR-10 is small). \n",
    "* `DataLoader` objects handle creating batches and iterating over the data.\n",
    "   \n",
    "Let's create some loaders for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to build a model that is able to successfully classify these images into their right category. To achieve this we will use Convolutional Neural Networks (CNNs).\n",
    "CNNs typically use, in their architecture, Convolutional layers:\n",
    "\n",
    "![Convolution image](https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_02_17A-ConvolutionalNeuralNetworks-WHITEBG.png)\n",
    "\n",
    "And Pooling layers:\n",
    "\n",
    "![Max pooling image](https://pyimagesearch.com/wp-content/uploads/2021/05/Convolutional-Neural-Networks-CNNs-and-Layer-Types.png)\n",
    "\n",
    "See the Machine Learning Primer for further details on how these layers work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the upcoming code we implement a simple CNN to classify our images. Since we are starting to work with bigger datasets and bigger networks it now makes sense to use a GPU (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# build the model (code partially from pytorch.org docs)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_labels):  \n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(  # runs each layer sequentially\n",
    "            nn.Conv2d(input_channels, 6, 5),\n",
    "            nn.ReLU(),  # a different activation layer than sigmoid\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*5*5, output_labels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "    \n",
    "simple_cnn = SimpleCNN(\n",
    "    input_channels=3,  # we have 3 color channels\n",
    "    output_labels=10  # we have 10 output categories\n",
    ").to(device)  # put the model in the GPU if available\n",
    "\n",
    "simple_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined our first CNN. It will take our image input, and output 10 values: one for each category. A high value means the network thinks the corresponding class is very likely, so all we have to do is pick the highest-scoring class to get a prediction.\n",
    "\n",
    "We can now train the network. Given the size of the dataset, the training will take longer than our previous 'synthetic' examples. However, the training procedure is the same as before, with the only exception that we use a different loss function: the *cross-entropy loss*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(simple_cnn.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "n_epochs = 2\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "simple_cnn.train()  # set model in training mode\n",
    "\n",
    "# loop over the dataset multiple times, similar to our \"steps\" used before\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        # don't forget to move the data to the right device\n",
    "        # also, data will be freed from memory once out of scope\n",
    "        outputs = simple_cnn(images.to(device))\n",
    "        loss = loss_fn(outputs, labels.to(device))\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += float(loss.item())\n",
    "        if i % 1000 == 999:\n",
    "            print(f'Epoch: {epoch+1} \\t Batch: {i+1} \\t Loss: {running_loss / 1000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "simple_cnn.eval()\n",
    "\n",
    "correct_samples = 0\n",
    "total_samples = 0\n",
    "\n",
    "Y_pred = []  # our model's predictions, will be used later\n",
    "Y_true = []  # true predictions, will be used later\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        outputs = simple_cnn(images.to(device))\n",
    "\n",
    "        # the class with the highest value is what we choose as prediction\n",
    "        _, predicted_index = torch.max(outputs.data, dim=1)  # gets max for each image and returns its value and index\n",
    "        \n",
    "        Y_true.extend(list(labels.numpy()))\n",
    "        Y_pred.extend(list(predicted_index.cpu().numpy()))\n",
    "        \n",
    "        total_samples += labels.to(device).size(0)\n",
    "        correct_samples += (predicted_index == labels.to(device)).sum().item()\n",
    "\n",
    "print(f'The test accuracy is {100 * correct_samples // total_samples:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got about 50% accuracy. Not that impressive, right? Well, given that we have ten classes, a random guesser would have 10% accuracy! We can safely say that the model was able to learn at least something.\n",
    "\n",
    "*__Note__: we have to set the model in training and evaluation mode for some layers to behave in their intended manner. It doesn't make a difference now but it will for more complex networks that use e.g. dropout, batch normalization, etc.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training models for multi-class classification, the overall accuracy score is still a great tool, it only tells a fraction of the story. To get a better sense of how well our model is doing, we can additionally look at its *confusion matrix*.\n",
    "\n",
    "The *confusion matrix* $C$ is a pivot table of the predicted label over the true label. Each row (index $i$) corresponds to one class for the true label $Y$, and each column (index $j$) corresponds to one class for the predicted label $Y_\\textrm{pred}$. The cell $C_{ij}$ contains the number of samples with true label $i$ and prediction $j$. The elements in the diagonal, $C_{ii}$, tell us how many samples were correctly classified for each class. The elements outside the diagonal tell us how many samples of a class were *confused* for another class (hence the name). This allows us to detect e.g. if the model is having trouble with one specific class ($C_{ii}$ is low), or the model tends to confuse a class for another ($C_{ij}$ is high for a specific $i\\neq j$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categories = ['Plane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    Y_true,\n",
    "    Y_pred, \n",
    "    normalize=\"pred\",\n",
    "    display_labels=categories,\n",
    "    values_format=\".2f\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this plot, the hardest class to detect are cats (lowest value in the diagonal). They are most commonly confused for dogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Save and Load the Model\n",
    "\n",
    "After spending so much time training this model, it may be worth saving it so that we don't need to repeat the whole process each time we want to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(simple_cnn.state_dict(), \"my_great_model_trained.pth\")\n",
    "\n",
    "# load the model\n",
    "new_cnn = SimpleCNN(3, 10)\n",
    "new_cnn.load_state_dict(torch.load('my_great_model_trained.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 14\n",
    "\n",
    "Extend our simple CNN with more `Linear` layers.\n",
    "\n",
    "**Step 1**: Create a `ComplexCNN` class that has the same architecture as our `SimpleCNN` one and takes as input `middle_layer`.\n",
    "\n",
    "**Step 2**: Change the last `Linear` layer to have output dimension of 120.\n",
    "\n",
    "**Step 3**: Add a `ReLU` layer, a `Linear` layer with out_dimension given by `middle_layer` defaulting to 84, another `ReLU` layer and finally another `Linear` one to output our category.\n",
    "\n",
    "**Step 4**: Train and evaluate the model on the CIFAR10 dataset. Print the final test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Build the model\n",
    "class ComplexCNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_labels, middle_layer=84):  \n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(  # runs each layer sequentially\n",
    "            nn.Conv2d(input_channels, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, middle_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(middle_layer, output_labels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "    \n",
    "complex_cnn = ComplexCNN(\n",
    "    input_channels=3,\n",
    "    output_labels=10,\n",
    "    middle_layer=84\n",
    ").to(device)\n",
    "\n",
    "complex_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Create the training loop\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(complex_cnn.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "n_epochs = 2\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "complex_cnn.train()  # set model in training mode\n",
    "\n",
    "# loop over the dataset multiple times, similar to our \"steps\" used before\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        # don't forget to move the data to the device\n",
    "        # also, data will be freed from memory once out of scope\n",
    "        outputs = complex_cnn(images.to(device))\n",
    "        loss = loss_fn(outputs, labels.to(device))\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Evaluate the model\n",
    "complex_cnn.eval()\n",
    "\n",
    "correct_samples = 0\n",
    "total_samples = 0\n",
    "\n",
    "Y_pred = []  # our model's predictions, will be used later\n",
    "Y_true = []  # true predictions, will be used later\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        outputs = complex_cnn(images.to(device))\n",
    "\n",
    "        # the class with the highest value is what we choose as prediction\n",
    "        _, predicted_index = torch.max(outputs.data, dim=1)  # gets max for each image and returns its value and index\n",
    "        \n",
    "        Y_true.extend(list(labels.numpy()))\n",
    "        Y_pred.extend(list(predicted_index.cpu().numpy()))\n",
    "        \n",
    "        total_samples += labels.to(device).size(0)\n",
    "        correct_samples += (predicted_index == labels.to(device)).sum().item()\n",
    "\n",
    "print(f'The test accuracy is {100 * correct_samples // total_samples:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 15\n",
    "Plot the confusion matrix for `complex_cnn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    Y_true,\n",
    "    Y_pred, \n",
    "    normalize=\"pred\",\n",
    "    display_labels=categories\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 16\n",
    "Save and load the `ComplexCNN` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(complex_cnn.state_dict(), \"my_great_complex_model_trained.pth\")\n",
    "\n",
    "# load the model\n",
    "new_cnn = ComplexCNN(3, 10, middle_layer=84)\n",
    "new_cnn.load_state_dict(torch.load('my_great_complex_model_trained.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Sidenote:__ One aspect of training that we didn't cover yet is hyper-parameter tuning. Varying the batch size, learning rate, number of units per layer, etc. will have a direct effect on the performance of your model. If you have the time, try changing the following parts of your model: (1) Increase the dimension of one or more `Linear` layer. (2) Try adding a `nn.Dropout2d(p=0.1)` between the Linear layers and experimenting with different values of p. (3) Decrease the learning rate. Then, re-train your model and check the accuracy.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Cross-Validation\n",
    "\n",
    "We have talked in the **ML Primer** about how models might \"memorize\" the training data too closely, and that is why we use separate train, validation and test sets to evaluate their performance. The ability for a model to respond well to new data is refered to as *generalization*. This is somewhat captured by the test-set accuracy, but can we quantify it better?\n",
    "\n",
    "One option is to train the model several times using different data. This way, we obtain several estimates of the accuracy, and we can see if the training process is stable (similar results, low variance) or unstable (distinct results, high variance). Using completely separate samples would be very wasteful, but there is a more economic solution: *$K$-fold cross-validation*. It works as follows:\n",
    "\n",
    "1. Split the whole dataset into $K$ *folds* (chunks of samples).\n",
    "2. For each fold:\n",
    "    1. Reserve the fold, and join all other folds into a training set.\n",
    "    2. Train on the joined set, test on the reserved fold.\n",
    "\n",
    "This gives $K$ separate estimates of the accuracy, so we can obtain an *average accuracy* and an *accuracy variance*. A high average means the model tends to produce good predictions; a low variance means the model generalizes well.\n",
    "\n",
    "Another thing we can do with cross-validation is hyper-parameter tuning where we test one model for each combination of our folds.\n",
    "\n",
    "Evaluating the accuracy using cross-validation in `sklearn` is quite easy, and you can find a [great practical overview](https://scikit-learn.org/stable/modules/cross_validation.html) over various types of cross-validation on their website.\n",
    "\n",
    "Let's use a cross-validation approach to decided the learning rate we're going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_batch(model, inputs, labels, optim):\n",
    "    model.train()\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return float(loss.item())\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_batch(model, inputs, labels):\n",
    "    model.eval()\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # the class with the highest value is what we choose as prediction\n",
    "    _, predicted_index = torch.max(outputs.data, dim=1)  # gets max for each image and returns its value and index\n",
    "    \n",
    "    return (predicted_index == labels).sum().item(), labels.size(0)\n",
    "    \n",
    "\n",
    "folds = 5 # number of chunks to split the dataset into\n",
    "\n",
    "learning_rates = [0.1 / 10**x for x in range (folds)]\n",
    "\n",
    "for i, (train_indices, val_indices) in enumerate(\n",
    "        KFold(n_splits=folds, shuffle=True).split(train_data.data, train_data.targets)\n",
    "    ):  \n",
    "    \n",
    "    train = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.Subset(train_data, train_indices),\n",
    "    )\n",
    "    val = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.Subset(train_data, val_indices)\n",
    "    )\n",
    "    \n",
    "    model = SimpleCNN(3, 10).to(device)\n",
    "    lr = learning_rates[i]\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss = []\n",
    "    \n",
    "    # train only one epoch\n",
    "    for images, labels in train:\n",
    "        loss.append(train_batch(model, images.to(device), labels.to(device), optim))\n",
    "\n",
    "    # eval. No need to use no_grad since it surrounds eval_batch\n",
    "    correct_samples, total_samples = 0, 0\n",
    "    for images, labels in val:\n",
    "        correct, total = eval_batch(model, images.to(device), labels.to(device))\n",
    "        correct_samples += correct\n",
    "        total_samples += total\n",
    "        \n",
    "    print(f\"Accuracy with {lr} as learning rate: {100 * correct_samples // total_samples:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found out that the best learning rate is 0.001. This is expected as we have too few steps for lower values to have an immidiate effect while bigger values only result in big oscillations in the weights.\n",
    "\n",
    "*__Note__: A good practice when using cross-validation for hyper-parameter tuning is to use nested cross-validations. In this way we have a inner cross-validation for a single model for which we can get better accuracy estimation and an outer validation to find the best model (relative to a hyper-parameter).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 17\n",
    "Use cross-validation to find the best value among `[20, 80, 160]` for the parameter to pass to `ComplexCNN`.\n",
    "\n",
    "(Remember we left the output dimension of one linear layer as a init parameter?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3  # number of chunks to split the dataset into\n",
    "\n",
    "neurons = [20, 80, 160]\n",
    "\n",
    "for i, (train_indices, val_indices) in enumerate(KFold(n_splits=folds, shuffle=True).split(train_data.data, train_data.targets)):  \n",
    "    train = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.Subset(train_data, train_indices),\n",
    "    )\n",
    "    val = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.Subset(train_data, val_indices)\n",
    "    )\n",
    "    \n",
    "    model = ComplexCNN(3, 10, neurons[i]).to(device)\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    loss = []\n",
    "    \n",
    "    # train only one epoch\n",
    "    for images, labels in train:\n",
    "        loss.append(train_batch(model, images.to(device), labels.to(device), optim))\n",
    "\n",
    "    # eval\n",
    "    correct_samples, total_samples = 0, 0\n",
    "    for images, labels in val:\n",
    "        correct, total = eval_batch(model, images.to(device), labels.to(device))\n",
    "        correct_samples += correct\n",
    "        total_samples += total\n",
    "        \n",
    "    print(f\"Accuracy with {neurons[i]} as neurons in the middle layer: {100 * correct_samples // total_samples:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2340f527b5fece118c83ac716b00ea0c393fef44aae5b0ffcfd166ef42dff93c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
