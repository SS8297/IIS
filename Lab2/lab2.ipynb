{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# run block of code and catch warnings\n",
    "with warnings.catch_warnings():\n",
    " # ignore all caught warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Emotion         1         2         3         4         5         6  \\\n",
      "0   NEGATIVE  0.915262  0.744733  0.146054  0.144566  0.461864  0.604585   \n",
      "1   NEGATIVE  0.884081  0.396989  0.169706  0.142127  0.656049  0.600000   \n",
      "2   NEGATIVE  0.856672  0.305868  0.122347  0.134102  0.449069  0.693600   \n",
      "3   NEGATIVE  0.773486  0.209016  0.140429  0.127732  0.452869  0.592668   \n",
      "4   NEGATIVE  0.830810  0.374441  0.159301  0.155777  0.566022  0.637202   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "63   NEUTRAL  0.864904  0.531306  0.143059  0.150276  0.611580  0.564786   \n",
      "64   NEUTRAL  0.781025  0.240832  0.092195  0.067082  0.676757  0.588218   \n",
      "65   NEUTRAL  0.817953  0.274721  0.045164  0.045164  0.594038  0.678962   \n",
      "66   NEUTRAL  0.731220  0.268802  0.038014  0.038014  0.462464  0.618828   \n",
      "67   NEUTRAL  0.687594  0.234346  0.133712  0.127163  0.624924  0.609501   \n",
      "\n",
      "           7         8         9  ...        13        14        15        16  \\\n",
      "0   0.846360  1.064337  2.364236  ...  1.036545  0.122506  0.078266  0.378601   \n",
      "1   0.861427  1.102822  2.018057  ...  0.521988  0.189618  0.192923  0.549770   \n",
      "2   0.795029  1.292486  1.819433  ...  0.269119  0.027185  0.019223  0.519015   \n",
      "3   0.438836  1.103530  1.644665  ...  0.222007  0.126496  0.144706  0.517902   \n",
      "4   0.951960  1.584929  1.849088  ...  0.445621  0.067864  0.055206  0.491030   \n",
      "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
      "63  1.204706  1.124483  2.088808  ...  0.231861  0.152471  0.162839  0.665697   \n",
      "64  0.918534  0.954521  1.646481  ...  0.556009  0.065526  0.062164  0.643360   \n",
      "65  0.657723  1.087968  1.739109  ...  0.275354  0.000000  0.000000  0.461644   \n",
      "66  0.668247  0.980581  1.558105  ...  0.327624  0.129505  0.129505  0.377568   \n",
      "67  0.846004  0.923548  1.457708  ...  0.259390  0.145600  0.146362  0.468468   \n",
      "\n",
      "          17        18        19        20        21        22  \n",
      "0   0.596056  1.266536  1.549980  3.056566  1.839942  0.708729  \n",
      "1   0.596808  0.594861  0.986486  1.847141  1.025906  0.241717  \n",
      "2   0.635515  0.718765  0.995882  1.871524  1.070320  0.317030  \n",
      "3   0.608588  0.418248  0.974367  1.484060  0.764261  0.177398  \n",
      "4   0.610774  0.784058  1.096521  2.036159  1.210408  0.355309  \n",
      "..       ...       ...       ...       ...       ...       ...  \n",
      "63  0.576826  0.950230  0.943669  2.083495  1.002995  0.501360  \n",
      "64  0.581672  1.363653  1.508948  2.105611  1.103012  0.708169  \n",
      "65  0.685027  0.988451  0.750000  1.748194  1.002603  0.516133  \n",
      "66  0.626461  0.535340  1.264911  1.419685  1.091885  0.262227  \n",
      "67  0.637318  0.805289  1.023938  1.457053  0.944277  0.383966  \n",
      "\n",
      "[68 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "    warnings.filterwarnings(\"ignore\")\n",
    "    data = pd.read_csv(\"Face-Feat.txt\")\n",
    "    print(data)\n",
    "    labels = data[\"Emotion\"]\n",
    "    inputs = data.drop(\"Emotion\", axis = 1)\n",
    "    data_in, test_in, data_out, test_out = train_test_split(inputs, labels, test_size=0.1, stratify=labels)\n",
    "    train_in, val_in, train_out, val_out = train_test_split(data_in, data_out, test_size = 0.2/0.9, stratify = data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy is  78.57142857142857\n"
     ]
    }
   ],
   "source": [
    "    model = SVC()\n",
    "    model.fit(train_in, train_out)\n",
    "    output = model.predict(val_in)\n",
    "    print(\"Val accuracy is \", accuracy_score(val_out, output)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy is  64.28571428571429\n"
     ]
    }
   ],
   "source": [
    "    model_2 = DecisionTreeClassifier()\n",
    "    model_2.fit(train_in, train_out)\n",
    "    output = model_2.predict(val_in)\n",
    "    print(\"Val accuracy is \", accuracy_score(val_out, output)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy is  71.42857142857143\n",
      "{'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "    param_grid=[\n",
    "        {\"kernel\": [\"linear\",\"poly\",\"rbf\"]},\n",
    "        {\"kernel\": [\"poly\"], \"degree\": [2, 5, 15]}\n",
    "    ]\n",
    "    meta_model = GridSearchCV(SVC(), param_grid=param_grid)\n",
    "    meta_model.fit(train_in, train_out)\n",
    "    output = meta_model.predict(test_in)\n",
    "    print(\"Val accuracy is \", accuracy_score(test_out, output)*100)\n",
    "    print(meta_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.net(nn.Sequential(\n",
    "            nn.Linear(22, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 3)\n",
    "        ))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return self.net(inputs)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEmoVA(Dataset):\n",
    "    def __init__(self, data_path) -> None:\n",
    "        super().__init__()\n",
    "        data=pd.read_csv(\"Face-Feat.txt\")\n",
    "        self.inputs = torch.tensor(data.drop(\"Emotion\", axis=1).to_numpy(np.float32))\n",
    "        self.index2label = [label for label in data[\"Emotion\"].unique()]\n",
    "        label2index = {label: i for i, label in enumerate(self.index2label)}\n",
    "        self.labels = torch.tensor(data[\"Emotion\"].apply(lambda x: label2index[x]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index], self.labels[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyNeuralNetwork' object has no attribute 'net'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m#print(data[0])\u001b[39;00m\n\u001b[0;32m      3\u001b[0m train, val, test \u001b[39m=\u001b[39m random_split(data, [\u001b[39m0.7\u001b[39m, \u001b[39m0.2\u001b[39m, \u001b[39m0.1\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m MyNeuralNetwork()\n\u001b[0;32m      6\u001b[0m \u001b[39m#print(model(train[0][0]).softmax(dim=0).argmax(dim=0))\u001b[39;00m\n\u001b[0;32m      8\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train)\n",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m, in \u001b[0;36mMyNeuralNetwork.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m----> 5\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m      6\u001b[0m         nn\u001b[39m.\u001b[39mLinear(\u001b[39m22\u001b[39m, \u001b[39m100\u001b[39m),\n\u001b[0;32m      7\u001b[0m         nn\u001b[39m.\u001b[39mReLU(),\n\u001b[0;32m      8\u001b[0m         nn\u001b[39m.\u001b[39mLinear(\u001b[39m100\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m      9\u001b[0m     ))\n",
      "File \u001b[1;32mc:\\Users\\shuai\\.julia\\conda\\3\\x86_64\\lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MyNeuralNetwork' object has no attribute 'net'"
     ]
    }
   ],
   "source": [
    "data = MultiEmoVA(\"Face-Feat.txt\")\n",
    "#print(data[0])\n",
    "train, val, test = random_split(data, [0.7, 0.2, 0.1])\n",
    "\n",
    "model = MyNeuralNetwork()\n",
    "#print(model(train[0][0]).softmax(dim=0).argmax(dim=0))\n",
    "\n",
    "train_loader = DataLoader(train)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optim = torch.optim.ADAM(model.parameters(), lr = 0.01)\n",
    "\n",
    "for inputs, labels in train_loader:\n",
    "    output = model(inputs)\n",
    "\n",
    "    loss = loss_fn(output, labels)\n",
    "    #print(loss)\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "test_loader = DataLoader(test)\n",
    "\n",
    "for inputs, labels, in test_loader:\n",
    "    output = model(inputs)\n",
    "    crrect += (ouput.softmax(dim=0).argmax(dim=1) == labels).sum()\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
